geom_line(aes(x = x, y = x5), color = "green") +
geom_line(aes(x = x, y = x6), color = "orange") +
geom_line(aes(x = x, y = x7), color = "navyblue") +
geom_line(aes(x = x, y = x8), color = "brown") +
geom_line(aes(x = x, y = x9), color = "grey") +
theme_minimal() +
labs(caption = "9 different Gaussian transformations of our original feature with different means and same variance", title = "Gaussian features - test data") + theme(plot.caption = element_text(hjust = 0.5))
require(glmnet) # we use this library to determine the optimal lambda
lambdas <- 10^seq(2, -2, by = -0.1)
ridge1 <- glmnet(df[1:20, ], as.numeric(df1[1:20, 2]),
alpha = 0, lambda = lambdas)
opt_lambda1 <- ridge1[[5]][length(ridge1[[5]])] # 20 observations is too little for  cross-validation thus the optimal lambda is chosen manually (looking which lambda gave the lowest variance)
cv.ridge2 <- cv.glmnet(df[1:40, ], as.numeric(df1[1:40, 2]),
alpha = 0, lambda = lambdas)
opt_lambda2 <- cv.ridge2$lambda.min
cv.ridge3 <- cv.glmnet(df[1:60, ], as.numeric(df1[1:60, 2]),
alpha = 0, lambda = lambdas)
opt_lambda3 <- cv.ridge3$lambda.min
cv.ridge4 <- cv.glmnet(df[1:80, ], as.numeric(df1[1:80, 2]),
alpha = 0, lambda = lambdas)
opt_lambda4 <- cv.ridge4$lambda.min
cv.ridge5 <- cv.glmnet(df, as.numeric(df1[, 2]),
alpha = 0, lambda = lambdas)
opt_lambda5 <- cv.ridge5$lambda.min
ridge <- function(mydata, lambda){
mydata <- cbind(intercept = rep(1, nrow(mydata)), mydata)
train_data <- unname(as.matrix(mydata[, - c(2:3)]))
w_ols_reg <- solve(lambda * diag(10) + t(train_data) %*% train_data) %*%
t(train_data) %*% mydata[, 3]
w_ols_reg <- as.data.frame(w_ols_reg, row.names = c(paste("w", 0:9, sep = '_')))
names(w_ols_reg) <- c("estimate")
return(w_ols_reg)
# return(vector(w_0 = w_ols_reg[1, 1], w_1 = w_ols_reg[2, 1], w_3 = w_ols_reg[3, 1],
#             w_4 = w_ols_reg[4, 1], w_5 = w_ols_reg[5, 1], w_6 = w_ols_reg[6, 1],
#             w_7 = w_ols_reg[7, 1], w_8 = w_ols_reg[8, 1], w_9 = w_ols_reg[9, 1]))
# option in case we wanted a named vector
}
rls1 <- ridge(df1[1:20, ], opt_lambda1)
fitted_rls1 <- df_test %*% as.matrix(rls1)
rls2 <- ridge(df1[1:40, ], opt_lambda2)
fitted_rls2 <- df_test %*% as.matrix(rls2)
rls3 <- ridge(df1[1:60, ], opt_lambda3)
fitted_rls3 <- df_test %*% as.matrix(rls3)
rls4 <- ridge(df1[1:80, ], opt_lambda4)
fitted_rls4 <- df_test %*% as.matrix(rls4)
rls5 <- ridge(df1, opt_lambda5)
fitted_rls5 <- df_test %*% as.matrix(rls5)
fitted_rls <- cbind(fitted_rls1, fitted_rls2, fitted_rls3,
fitted_rls4, fitted_rls5)
ridge_coef <- cbind(rls1, rls2, rls3, rls4, rls5)
names(ridge_coef) <- c("20 obs", "40 obs", "60 obs", "80 obs", "100 obs")
print(ridge_coef)
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls1), color = "red") +
geom_line(aes(x = x,
y = fitted_rls2), color = "blue") +
geom_line(aes(x = x,
y = fitted_rls3), color = "navyblue") +
geom_line(aes(x = x,
y = fitted_rls4), color = "green") +
geom_line(aes(x = x,
y = fitted_rls5), color = "orange") +
theme_minimal() + labs(title = "Ridge regression", caption = "Fitted values of our Ridge regression models trained on 5 different subsets observations") + theme(plot.caption = element_text(hjust = 0.5))
bayes_solution <- function(mydata, alpha, beta){
mydata <- unname(as.matrix(cbind(intercept = rep(1, nrow(mydata)), mydata[, -1])))
alphaI <- alpha * diag(10)
S <- solve(alphaI + beta * t(mydata[, -2]) %*% mydata[, -2])
m <- as.vector(beta * S %*%  t(mydata[, -2]) %*% mydata[, 2])
m <- as.data.frame(m, row.names = c(paste("w", 0:9, sep = '_')))
names(m) <- c("estimate")
return(m)
}
# we arbitrarily chose as prior alpha = 0 and beta equal 20, we can see the results are quite good
bayes1 <- bayes_solution(df1[1:20, ], 0, 20)
fitted_bayes1 <- df_test %*% as.matrix(bayes1)
bayes2 <- bayes_solution(df1[1:40, ], 0, 20)
fitted_bayes2 <- df_test %*% as.matrix(bayes2)
bayes3 <- bayes_solution(df1[1:60, ], 0, 20)
fitted_bayes3 <- df_test %*% as.matrix(bayes3)
bayes4 <- bayes_solution(df1[1:80, ], 0, 20)
fitted_bayes4 <- df_test %*% as.matrix(bayes4)
bayes5 <- bayes_solution(df1, 0, 20)
fitted_bayes5 <- df_test %*% as.matrix(bayes5)
fitted_bayes <- cbind(fitted_bayes1, fitted_bayes2, fitted_bayes3,
fitted_bayes4, fitted_bayes5)
bayes_coef <- cbind(bayes1, bayes2, bayes3, bayes4, bayes5)
names(bayes_coef) <- c("20 obs", "40 obs", "60 obs", "80 obs", "100 obs")
print(bayes_coef)
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_bayes1), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes2), color = "blue") +
geom_line(aes(x = x,
y = fitted_bayes3), color = "navyblue") +
geom_line(aes(x = x,
y = fitted_bayes4), color = "green") +
geom_line(aes(x = x,
y = fitted_bayes5), color = "orange") +
theme_minimal() + labs(title = "Bayesian regression", caption = "Fitted values of our Bayesian regression models trained on 5 different subsets observations") + theme(plot.caption = element_text(hjust = 0.5))
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls1), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes1), color = "navyblue") +
theme_minimal() + labs(title = "Ridge vs Bayesian regression", caption = "Fitted values of our Ridge regression and Bayesian regression model trained on 20 observations") + theme(plot.caption = element_text(hjust = 0.5))
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls5), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes5), color = "navyblue") +
theme_minimal() + labs(title = "Ridge vs Bayesian regression", caption = "Fitted values of our Ridge regression and Bayesian regression model trained on 100 observations") + theme(plot.caption = element_text(hjust = 0.5))
# table comparison
rmse_bayes <- vector()
for (i in 1:5){
rmse_bayes[i] <- sqrt(sum((fitted_bayes[, i] - target)^2 / length(target)))
}
rmse_rls <- vector()
for (i in 1:5){
rmse_rls[i] <- sqrt(sum((fitted_rls[, i] - target)^2 / length(target)))
}
comp <- data.frame(rmse_bayes, rmse_rls, row.names = c("20 observations",
"40 observations", "60 observations",
"80 observations", "100 observations"))
names(comp) <- c("Bayes RMSE", "Ridge RMSE")
print(comp)
# properly estimated bayesian regression (unlike in #3 we estimate also alpha, beta)
BLR_posterior <- function(phi, target, beta, m_N_old, S_N_old){
# the prior knowledge
m_0 <- m_N_old
S_0 <- S_N_old
# updating the prior knowledge
S_N <- solve(solve(S_0) + beta * t(phi) %*% phi)
m_N <- S_N %*% (solve(S_0) %*% m_0 + beta * t(phi) %*% target)
return(list(m_N, S_N))
}
BLR_optim <- function(phi, t, max_iter = 1000,
d_alpha_min = 0.01,
d_beta_min = 0.01){
alpha <- 0.5
beta <- 5
m_N <- as.matrix(rep(0, 10))
S_N <- alpha * diag(1, 10)
iteration <- 1
d_alpha <- 5
d_beta <- 5
# this loop determines the optimal values of alpha and beta
while ((iteration < max_iter) & ((abs(d_alpha) > d_alpha_min) | (abs(d_beta) > d_beta_min))) {
lambdas <- eigen(beta * t(phi) %*% phi)$values
gamma <- 0
for (lambda in lambdas) {
gamma <- gamma + (lambda / (alpha + lambda))
}
updated_mN_SN <- BLR_posterior(phi = phi,
target = t,
beta = beta,
m_N_old = m_N,
S_N_old = S_N)
m_N <- updated_mN_SN[[1]]
S_N <- updated_mN_SN[[2]]
SSR <- 0
for (i in 1:nrow(phi)) {
SSR <- SSR + (t[i] - (t(m_N) %*% phi[i, ]))^2
}
d_alpha <- (gamma / (t(m_N) %*% m_N)) - alpha
alpha <- as.numeric(alpha + d_alpha)
d_beta <- (1 / ((1 / (nrow(phi) - gamma)) * SSR)) - beta
beta <- as.numeric(beta + d_beta)
iteration <- iteration + 1
}
# now we estimate the BLR with the optimal values of alpha and beta
m_0 <- as.matrix(rep(0, 10))
S_0 <- alpha * diag(1, 10)
S_N <- solve(solve(S_0) + beta * t(phi) %*% phi)
m_N <- S_N %*% (solve(S_0) %*% m_0 + beta * t(phi) %*% t)
print(paste("The number of iterations is", iteration, sep = " "))
print(paste("The optimal value of hyperparameter alpha is", alpha, sep = " "))
print(paste("The optimal value of hyperparameter beta is", beta, sep = " "))
return(m_N)
}
BLR1 <- BLR_optim(df[1:20, ], df1$target[1:20])
fitted_BLR1 <- df_test %*% as.matrix(BLR1)
BLR2 <- BLR_optim(df[1:40, ], df1$target[1:40])
fitted_BLR2 <- df_test %*% as.matrix(BLR2)
BLR3 <- BLR_optim(df[1:60, ], df1$target[1:60])
fitted_BLR3 <- df_test %*% as.matrix(BLR3)
BLR4 <- BLR_optim(df[1:80, ], df1$target[1:80])
fitted_BLR4 <- df_test %*% as.matrix(BLR4)
BLR5 <- BLR_optim(df, df1$target)
fitted_BLR5 <- df_test %*% as.matrix(BLR5)
fitted_BLR <- cbind(fitted_BLR1, fitted_BLR2, fitted_BLR3,
fitted_BLR4, fitted_BLR5)
coef_BLR <- cbind(BLR1, BLR2, BLR3, BLR4, BLR5)
colnames(coef_BLR) <- c("20 obs", "40 obs", "60 obs", "80 obs", "100 obs")
print(coef_BLR)
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls1), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes1), color = "navyblue") +
geom_line(aes(x = x,
y = fitted_BLR1), color = "indianred") +
theme_minimal() + labs(title = "Ridge vs (optimal) Bayesian regression", caption = "Fitted values of our Ridge regression and (optimal) Bayesian regression model trained on 20 observations") + theme(plot.caption = element_text(hjust = 0.5))
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls5), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes5), color = "navyblue") +
geom_line(aes(x = x,
y = fitted_BLR5), color = "indianred") +
theme_minimal() + labs(title = "Ridge vs (optimal) Bayesian regression", caption = "Fitted values of our Ridge regression and (optimal) Bayesian regression model trained on 100 observations") + theme(plot.caption = element_text(hjust = 0.5))
rmse_BLR <- vector()
for (i in 1:5){
rmse_BLR[i] <- sqrt(sum((fitted_BLR[, i] - target)^2 / length(target)))
}
comp1 <- data.frame(rmse_BLR, rmse_rls, row.names = c("20 observations",
"40 observations", "60 observations",
"80 observations", "100 observations"))
names(comp1) <- c("BLR RMSE", "Ridge RMSE")
print(comp1)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
set.seed(150)
my_func <- function(n){
x <- runif(n, min = - 1, max = 1)
noise <- rnorm(n, mean = 0, sd = 0.1)
target <- sin(2 * pi * x) + noise
return(data.frame(x, target))
}
##### Generate data and iteration steps #####
my_values <- my_func(100)
target <- my_values$target
x <- my_values$x
my_values_test <- my_func(100)
target_test <- my_values_test$target
x_test <- my_values_test$x
ggplot(data = my_values, aes(x = x, y = target)) + geom_jitter() +
theme_minimal() + labs(title = "Train data", caption = "Target variable plotted against the single feature") + theme(plot.caption = element_text(hjust = 0.5))
ggplot(data = my_values_test, aes(x = x, y = target)) + geom_jitter() +
theme_minimal() +
labs(title = "Test data", caption = "Target variable plotted against the single feature") + theme(plot.caption = element_text(hjust = 0.5))
mean <- seq(- 1, 1, by = 0.25)
sd <- 0.15
gaus_basis <- function(mean, sd, x){
new_features <- as.data.frame(x)
for (i in 1:length(mean)){
new_features[, i] <- exp( - (x - mean[i])^2 / (2 * sd^2)) # dnorm(x, mean[i], sd)
}
names(new_features) <- c('x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9')
return(new_features)
}
# training data set with the transformed features
features1 <- gaus_basis(mean, sd, x)
df1 <- cbind(my_values, features1)
df <- cbind(rep(1, 100), unname(as.matrix(df1[, - c(1:2)]))) # include intercept
# testing data set with the transformed features
features1_test <- gaus_basis(mean, sd, x_test)
df1_test <- cbind(my_values_test, features1_test)
df_test <- cbind(rep(1, 100), unname(as.matrix(df1_test[, - c(1:2)]))) # include intercept
ggplot(data = df1) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x, y = x1), color = "red") +
geom_line(aes(x = x, y = x2), color = "indianred") +
geom_line(aes(x = x, y = x3), color = "blue") +
geom_line(aes(x = x, y = x4), color = "yellow") +
geom_line(aes(x = x, y = x5), color = "green") +
geom_line(aes(x = x, y = x6), color = "orange") +
geom_line(aes(x = x, y = x7), color = "navyblue") +
geom_line(aes(x = x, y = x8), color = "brown") +
geom_line(aes(x = x, y = x9), color = "grey") +
theme_minimal() +
labs(caption = "9 different Gaussian transformations of our original feature with different means and same variance", title = "Gaussian features - train data") + theme(plot.caption = element_text(hjust = 0.5))
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x, y = x1), color = "red") +
geom_line(aes(x = x, y = x2), color = "indianred") +
geom_line(aes(x = x, y = x3), color = "blue") +
geom_line(aes(x = x, y = x4), color = "yellow") +
geom_line(aes(x = x, y = x5), color = "green") +
geom_line(aes(x = x, y = x6), color = "orange") +
geom_line(aes(x = x, y = x7), color = "navyblue") +
geom_line(aes(x = x, y = x8), color = "brown") +
geom_line(aes(x = x, y = x9), color = "grey") +
theme_minimal() +
labs(caption = "9 different Gaussian transformations of our original feature with different means and same variance", title = "Gaussian features - test data") + theme(plot.caption = element_text(hjust = 0.5))
require(glmnet) # we use this library to determine the optimal lambda
lambdas <- 10^seq(2, -2, by = -0.1)
ridge1 <- glmnet(df[1:20, ], as.numeric(df1[1:20, 2]),
alpha = 0, lambda = lambdas)
opt_lambda1 <- ridge1[[5]][length(ridge1[[5]])] # 20 observations is too little for  cross-validation thus the optimal lambda is chosen manually (looking which lambda gave the lowest variance)
cv.ridge2 <- cv.glmnet(df[1:40, ], as.numeric(df1[1:40, 2]),
alpha = 0, lambda = lambdas)
opt_lambda2 <- cv.ridge2$lambda.min
cv.ridge3 <- cv.glmnet(df[1:60, ], as.numeric(df1[1:60, 2]),
alpha = 0, lambda = lambdas)
opt_lambda3 <- cv.ridge3$lambda.min
cv.ridge4 <- cv.glmnet(df[1:80, ], as.numeric(df1[1:80, 2]),
alpha = 0, lambda = lambdas)
opt_lambda4 <- cv.ridge4$lambda.min
cv.ridge5 <- cv.glmnet(df, as.numeric(df1[, 2]),
alpha = 0, lambda = lambdas)
opt_lambda5 <- cv.ridge5$lambda.min
ridge <- function(mydata, lambda){
mydata <- cbind(intercept = rep(1, nrow(mydata)), mydata)
train_data <- unname(as.matrix(mydata[, - c(2:3)]))
w_ols_reg <- solve(lambda * diag(10) + t(train_data) %*% train_data) %*%
t(train_data) %*% mydata[, 3]
w_ols_reg <- as.data.frame(w_ols_reg, row.names = c(paste("w", 0:9, sep = '_')))
names(w_ols_reg) <- c("estimate")
return(w_ols_reg)
# return(vector(w_0 = w_ols_reg[1, 1], w_1 = w_ols_reg[2, 1], w_3 = w_ols_reg[3, 1],
#             w_4 = w_ols_reg[4, 1], w_5 = w_ols_reg[5, 1], w_6 = w_ols_reg[6, 1],
#             w_7 = w_ols_reg[7, 1], w_8 = w_ols_reg[8, 1], w_9 = w_ols_reg[9, 1]))
# option in case we wanted a named vector
}
rls1 <- ridge(df1[1:20, ], opt_lambda1)
fitted_rls1 <- df_test %*% as.matrix(rls1)
rls2 <- ridge(df1[1:40, ], opt_lambda2)
fitted_rls2 <- df_test %*% as.matrix(rls2)
rls3 <- ridge(df1[1:60, ], opt_lambda3)
fitted_rls3 <- df_test %*% as.matrix(rls3)
rls4 <- ridge(df1[1:80, ], opt_lambda4)
fitted_rls4 <- df_test %*% as.matrix(rls4)
rls5 <- ridge(df1, opt_lambda5)
fitted_rls5 <- df_test %*% as.matrix(rls5)
fitted_rls <- cbind(fitted_rls1, fitted_rls2, fitted_rls3,
fitted_rls4, fitted_rls5)
ridge_coef <- cbind(rls1, rls2, rls3, rls4, rls5)
names(ridge_coef) <- c("20 obs", "40 obs", "60 obs", "80 obs", "100 obs")
print(ridge_coef)
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls1), color = "red") +
geom_line(aes(x = x,
y = fitted_rls2), color = "blue") +
geom_line(aes(x = x,
y = fitted_rls3), color = "navyblue") +
geom_line(aes(x = x,
y = fitted_rls4), color = "green") +
geom_line(aes(x = x,
y = fitted_rls5), color = "orange") +
theme_minimal() + labs(title = "Ridge regression", caption = "Fitted values of our Ridge regression models trained on 5 different subsets observations") + theme(plot.caption = element_text(hjust = 0.5))
bayes_solution <- function(mydata, alpha, beta){
mydata <- unname(as.matrix(cbind(intercept = rep(1, nrow(mydata)), mydata[, -1])))
alphaI <- alpha * diag(10)
S <- solve(alphaI + beta * t(mydata[, -2]) %*% mydata[, -2])
m <- as.vector(beta * S %*%  t(mydata[, -2]) %*% mydata[, 2])
m <- as.data.frame(m, row.names = c(paste("w", 0:9, sep = '_')))
names(m) <- c("estimate")
return(m)
}
# we arbitrarily chose as prior alpha = 0 and beta equal 20, we can see the results are quite good
bayes1 <- bayes_solution(df1[1:20, ], 0, 20)
fitted_bayes1 <- df_test %*% as.matrix(bayes1)
bayes2 <- bayes_solution(df1[1:40, ], 0, 20)
fitted_bayes2 <- df_test %*% as.matrix(bayes2)
bayes3 <- bayes_solution(df1[1:60, ], 0, 20)
fitted_bayes3 <- df_test %*% as.matrix(bayes3)
bayes4 <- bayes_solution(df1[1:80, ], 0, 20)
fitted_bayes4 <- df_test %*% as.matrix(bayes4)
bayes5 <- bayes_solution(df1, 0, 20)
fitted_bayes5 <- df_test %*% as.matrix(bayes5)
fitted_bayes <- cbind(fitted_bayes1, fitted_bayes2, fitted_bayes3,
fitted_bayes4, fitted_bayes5)
bayes_coef <- cbind(bayes1, bayes2, bayes3, bayes4, bayes5)
names(bayes_coef) <- c("20 obs", "40 obs", "60 obs", "80 obs", "100 obs")
print(bayes_coef)
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_bayes1), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes2), color = "blue") +
geom_line(aes(x = x,
y = fitted_bayes3), color = "navyblue") +
geom_line(aes(x = x,
y = fitted_bayes4), color = "green") +
geom_line(aes(x = x,
y = fitted_bayes5), color = "orange") +
theme_minimal() + labs(title = "Bayesian regression", caption = "Fitted values of our Bayesian regression models trained on 5 different subsets observations") + theme(plot.caption = element_text(hjust = 0.5))
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls1), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes1), color = "navyblue") +
theme_minimal() + labs(title = "Ridge vs Bayesian regression", caption = "Fitted values of our Ridge regression and Bayesian regression model trained on 20 observations") + theme(plot.caption = element_text(hjust = 0.5))
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls5), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes5), color = "navyblue") +
theme_minimal() + labs(title = "Ridge vs Bayesian regression", caption = "Fitted values of our Ridge regression and Bayesian regression model trained on 100 observations") + theme(plot.caption = element_text(hjust = 0.5))
# table comparison
rmse_bayes <- vector()
for (i in 1:5){
rmse_bayes[i] <- sqrt(sum((fitted_bayes[, i] - target)^2 / length(target)))
}
rmse_rls <- vector()
for (i in 1:5){
rmse_rls[i] <- sqrt(sum((fitted_rls[, i] - target)^2 / length(target)))
}
comp <- data.frame(rmse_bayes, rmse_rls, row.names = c("20 observations",
"40 observations", "60 observations",
"80 observations", "100 observations"))
names(comp) <- c("Bayes RMSE", "Ridge RMSE")
print(comp)
# properly estimated bayesian regression (unlike in #3 we estimate also alpha, beta)
BLR_posterior <- function(phi, target, beta, m_N_old, S_N_old){
# the prior knowledge
m_0 <- m_N_old
S_0 <- S_N_old
# updating the prior knowledge
S_N <- solve(solve(S_0) + beta * t(phi) %*% phi)
m_N <- S_N %*% (solve(S_0) %*% m_0 + beta * t(phi) %*% target)
return(list(m_N, S_N))
}
BLR_optim <- function(phi, t, max_iter = 1000,
d_alpha_min = 0.01,
d_beta_min = 0.01){
alpha <- 0.5
beta <- 5
m_N <- as.matrix(rep(0, 10))
S_N <- alpha * diag(1, 10)
iteration <- 1
d_alpha <- 5
d_beta <- 5
# this loop determines the optimal values of alpha and beta
while ((iteration < max_iter) & ((abs(d_alpha) > d_alpha_min) | (abs(d_beta) > d_beta_min))) {
lambdas <- eigen(beta * t(phi) %*% phi)$values
gamma <- 0
for (lambda in lambdas) {
gamma <- gamma + (lambda / (alpha + lambda))
}
updated_mN_SN <- BLR_posterior(phi = phi,
target = t,
beta = beta,
m_N_old = m_N,
S_N_old = S_N)
m_N <- updated_mN_SN[[1]]
S_N <- updated_mN_SN[[2]]
SSR <- 0
for (i in 1:nrow(phi)) {
SSR <- SSR + (t[i] - (t(m_N) %*% phi[i, ]))^2
}
d_alpha <- (gamma / (t(m_N) %*% m_N)) - alpha
alpha <- as.numeric(alpha + d_alpha)
d_beta <- (1 / ((1 / (nrow(phi) - gamma)) * SSR)) - beta
beta <- as.numeric(beta + d_beta)
iteration <- iteration + 1
}
# now we estimate the BLR with the optimal values of alpha and beta
m_0 <- as.matrix(rep(0, 10))
S_0 <- alpha * diag(1, 10)
S_N <- solve(solve(S_0) + beta * t(phi) %*% phi)
m_N <- S_N %*% (solve(S_0) %*% m_0 + beta * t(phi) %*% t)
print(paste("The number of iterations is", iteration, sep = " "))
print(paste("The optimal value of hyperparameter alpha is", alpha, sep = " "))
print(paste("The optimal value of hyperparameter beta is", beta, sep = " "))
return(m_N)
}
BLR1 <- BLR_optim(df[1:20, ], df1$target[1:20])
fitted_BLR1 <- df_test %*% as.matrix(BLR1)
BLR2 <- BLR_optim(df[1:40, ], df1$target[1:40])
fitted_BLR2 <- df_test %*% as.matrix(BLR2)
BLR3 <- BLR_optim(df[1:60, ], df1$target[1:60])
fitted_BLR3 <- df_test %*% as.matrix(BLR3)
BLR4 <- BLR_optim(df[1:80, ], df1$target[1:80])
fitted_BLR4 <- df_test %*% as.matrix(BLR4)
BLR5 <- BLR_optim(df, df1$target)
fitted_BLR5 <- df_test %*% as.matrix(BLR5)
fitted_BLR <- cbind(fitted_BLR1, fitted_BLR2, fitted_BLR3,
fitted_BLR4, fitted_BLR5)
coef_BLR <- cbind(BLR1, BLR2, BLR3, BLR4, BLR5)
colnames(coef_BLR) <- c("20 obs", "40 obs", "60 obs", "80 obs", "100 obs")
print(coef_BLR)
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls1), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes1), color = "navyblue") +
geom_line(aes(x = x,
y = fitted_BLR1), color = "indianred") +
theme_minimal() + labs(title = "Ridge vs (optimal) Bayesian regression", caption = "Fitted values of our Ridge regression and (optimal) Bayesian regression model trained on 20 observations") + theme(plot.caption = element_text(hjust = 0.5))
ggplot(data = df1_test) +
geom_jitter(aes(x = x, y = target)) +
geom_line(aes(x = x,
y = fitted_rls5), color = "red") +
geom_line(aes(x = x,
y = fitted_bayes5), color = "navyblue") +
geom_line(aes(x = x,
y = fitted_BLR5), color = "indianred") +
theme_minimal() + labs(title = "Ridge vs (optimal) Bayesian regression", caption = "Fitted values of our Ridge regression and (optimal) Bayesian regression model trained on 100 observations") + theme(plot.caption = element_text(hjust = 0.5))
rmse_BLR <- vector()
for (i in 1:5){
rmse_BLR[i] <- sqrt(sum((fitted_BLR[, i] - target)^2 / length(target)))
}
comp1 <- data.frame(rmse_BLR, rmse_rls, row.names = c("20 observations",
"40 observations", "60 observations",
"80 observations", "100 observations"))
names(comp1) <- c("BLR RMSE", "Ridge RMSE")
print(comp1)
