VbQML <- A1 %*% B %*% A1 / N
sebML <- sqrt(diag(VbML))
sebQML <- sqrt(diag(VbQML))
VbML
VbQML
sebML
sebQML
?nls
# c
fitNLS <- nls(y ~ exp(X), start = theta)
# c
fitNLS <- nls(y ~ exp(X[, 1] * theta1 + X[, 2] * theta2), start = list(theta1 = b[1], theta2 = b[2]))
fitNLS
fitNLS$m$conv()
fitNLS$convInfo
fitNLS$control
fitNLS$data
fitNLS$call
fitNLS$data
fitNLS$m$Rmat()
summary(fitNLS)
summary(fitML)
?dexp
?rep
sample(0, 2, replace = T)
R <- 1000
b_ML <- matrix(sample(0, R * 2, replace = T), R, 2)
b_NLS <- matrix(sample(0, R * 2, replace = T), R, 2)
for (i in 1:R) {
y1 <- rexp(N, rate = 1 / exp(x - 2))
data1 <- cbind(y1, X)
colnames(data) <- c("y", "x0", "x1")
b_ML[i, ] <- mle2(y1 ~ dexp(rate = exp(x0 * theta1 + x1 * theta2)), start = list(theta1 = 2, theta2 = - 1),
data = as.data.frame(data1))
b_NLS[i, ] <- nls(y1 ~ exp(X[, 1] * theta1 + X[, 2] * theta2), start = list(theta1 = 2, theta2 = - 1))
}
for (i in 1:R) {
y1 <- rexp(N, rate = 1 / exp(x - 2))
data1 <- cbind(y1, X)
colnames(data1) <- c("y", "x0", "x1")
b_ML[i, ] <- mle2(y1 ~ dexp(rate = exp(x0 * theta1 + x1 * theta2)), start = list(theta1 = 2, theta2 = - 1),
data = as.data.frame(data1))
b_NLS[i, ] <- nls(y1 ~ exp(X[, 1] * theta1 + X[, 2] * theta2), start = list(theta1 = 2, theta2 = - 1))
}
fitML
fitML@coef
coef(mle2(y1 ~ dexp(rate = exp(x0 * theta1 + x1 * theta2)), start = list(theta1 = 2, theta2 = - 1),
data = as.data.frame(data1)))
for (i in 1:R){
y1 <- rexp(N, rate = 1 / exp(x - 2))
data1 <- cbind(y1, X)
colnames(data1) <- c("y", "x0", "x1")
b_ML[i, ] <- coef(mle2(y1 ~ dexp(rate = exp(x0 * theta1 + x1 * theta2)), start = list(theta1 = 2, theta2 = - 1),
data = as.data.frame(data1)))
b_NLS[i, ] <- coefficients(nls(y1 ~ exp(X[, 1] * theta1 + X[, 2] * theta2),
start = list(theta1 = 2, theta2 = - 1)))
}
mean(b_ML)
dim(b_ML)
mean(b_ML[, 1])
mean(b_ML[, 2])
print(mean(b_ML[, 1]), mean(b_ML[, 2]), mean(b_NLS[, 1]). mean(b_NLS[, 2]))
print(mean(b_ML[, 1]), mean(b_ML[, 2]), mean(b_NLS[, 1]), mean(b_NLS[, 2]))
mean(b_ML[, 1]
mean(b_ML[, 1])
mean(b_ML[, 2])
mean(b_NLS[, 1])
mean(b_NLS[, 2]
)
fitNLS
# c
fitNLS <- nls(y ~ - exp(X[, 1] * theta1 + X[, 2] * theta2), start = list(theta1 = b[1], theta2 = b[2]))
fitNLS
# c
fitNLS <- nls(y ~  exp(- (X[, 1] * theta1 + X[, 2] * theta2)), start = list(theta1 = b[1], theta2 = b[2]))
fitNLS
print(mean(b_ML[, 1]), mean(b_ML[, 2]), mean(b_NLS[, 1]), mean(b_NLS[, 2]))
mean(b_ML[, 1])
mean(b_ML[, 2])
mean(b_NLS[, 1])
mean(b_NLS[, 2]))
mean(b_NLS[, 2])
beta <- 1
rho <- 0.5
r <- 3
pi_1 <- 1
pi <- c(pi_1, rep(0, r - 1))
pi
gamma_1 <- 0 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
?norm
?rnorm
cbind(rep(rnorm(N, 0, 1), r))
head(cbind(rep(rnorm(N, 0, 1), r)))
head(rep(rnorm(N, 0, 1), r))
beta <- 1
rho <- 0.5
r <- 3
pi_1 <- 1
pi <- c(pi_1, rep(0, r - 1))
gamma_1 <- 0 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
rep(rnorm(N, 0, 1), r)
N <- 100
rep(rnorm(N, 0, 1), r)
head(rep(rnorm(N, 0, 1), r))
dim(rep(rnorm(N, 0, 1), r))
length(rep(rnorm(N, 0, 1), r))
Z <- cbind(rnorm(N, 0, 1), r), rnorm(N, 0, 1), r), rnorm(N, 0, 1), r))
Z <- matrix(rnorm(N, 0, 1), N, r)
sigma <- exp(Z %*% gamma)
R <- 200
b_2SLS <- matrix(sample(0, R, replace = T), R, 1)
b_GMM <- matrix(sample(0, R, replace = T), R, 1)
b_EGMM <- matrix(sample(0, R, replace = T), R, 1)
b_OLS <- matrix(sample(0, R, replace = T), R, 1)
beta <- 1
rho <- 0.5
r <- 3
pi_1 <- 1
pi <- c(pi_1, rep(0, r - 1))
gamma_1 <- 0 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
N <- 100
Z <- matrix(rnorm(N, 0, 1), N, r)
sigma <- exp(Z %*% gamma)
# matrices for results
R <- 200
b_2SLS <- matrix(sample(0, R, replace = T), R, 1)
b_GMM <- matrix(sample(0, R, replace = T), R, 1)
b_EGMM <- matrix(sample(0, R, replace = T), R, 1)
b_OLS <- matrix(sample(0, R, replace = T), R, 1)
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- lm(y ~ x)
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% Z %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- coef(lm(y ~ x))
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% Z %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
coef(lm(y ~ x))
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- coef(lm(y ~ x))[2]
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% Z %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
t(Z) %*% Z
Z <- cbin(rnorm(N, 0, 1), rnorm(N, 0, 1), rnorm(N, 0, 1))
Z <- cbind(rnorm(N, 0, 1), rnorm(N, 0, 1), rnorm(N, 0, 1))
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- coef(lm(y ~ x))[2]
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% Z %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
t(Z) %*% Z
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- coef(lm(y ~ x))[2]
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(b_OLS)
coef(lm(y ~ x))[2]
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
as.numeric(coef(lm(y ~ x))[2])
b_2SLS <- matrix(sample(0, R, replace = T), R, 1)
b_GMM <- matrix(sample(0, R, replace = T), R, 1)
b_EGMM <- matrix(sample(0, R, replace = T), R, 1)
b_OLS <- matrix(sample(0, R, replace = T), R, 1)
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(b_OLS)
mean(b_2SLS)
gamma_1 <- 0.5 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
N <- 100
Z <- cbind(rnorm(N, 0, 1), rnorm(N, 0, 1), rnorm(N, 0, 1))
sigma <- exp(Z %*% gamma)
# matrices for results
R <- 200
b_2SLS <- matrix(sample(0, R, replace = T), R, 1)
b_GMM <- matrix(sample(0, R, replace = T), R, 1)
b_OLS <- matrix(sample(0, R, replace = T), R, 1)
b_EGMM <- matrix(sample(0, R, replace = T), R, 1)
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(b_2SLS)
mean(e)
gamma_1 <- 0 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(e)
gamma_1 <- 1 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(e)
library(tidyverse)
library(installr)
install.packages("installr")
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("dplyr")
library(tidyverse)
install.packages("tidyverse")
install.packages("tm")
install.packages("mgcv")
install.packages("readr")
install.packages("RCurl")
knitr::opts_chunk$set(echo = F,	message = FALSE,warning = FALSE)
library(tidyverse)
ggplot(data = iris, aes(x = Sepal.Width, fill = Species)) +
geom_histogram(binwidth = 0.2, col = "black", size = 1) +
labs(x = "Sepal width", y = "Frequency", title = "Histogram of Sepal width")
knitr::opts_chunk$set(echo = F,	message = FALSE,warning = FALSE)
ggplot(data = iris, aes(x = Sepal.Length, fill = Species, col = Species)) +
geom_density(alpha = 0.5, size = 0.5) +
labs(x = "Sepal length", y = "Density", title = "Iris dataset:
Smoothed density of Sepal Length per Species")
knitr::opts_chunk$set(echo = F,	message = FALSE,warning = FALSE)
ggplot(data = iris, aes(x = Sepal.Length, y = iris$Sepal.Width)) +
geom_point(aes(x = iris$Sepal.Length, y = iris$Sepal.Width, fill = iris$Species,
col = iris$Species, shape = iris$Species),
size = 2) +
geom_smooth(method = lm, aes(x = iris$Sepal.Length,
y = iris$Sepal.Width, col = iris$Species)) +
labs(x = "Sepal length", y = "Sepal Width", title = "Scatterplot with smoothers")
library(tm)
getSources()
setwd()
library(keras)
library(glmnet)
install.packages("dbscan")
install.packages("e1071")
library(dbscan)
library(e1071)
?dbscan
?svm
library(kernlab)
# Load other libraries
library(data.table)
library(ggplot2)
library(text2vec)
library(stringr)
library(tokenizers)
set.seed(150)
# loading data
amazon_sentences <- fread("sentiment labelled sentences/amazon_cells_labelled.txt",
header = FALSE, sep = "\t")
colnames(amazon_sentences) <- c("text", "sentiment")
amazon_sentences$id <- rownames(amazon_sentences)
# dtm
prep_fun <- function(x) {
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', ' ') # replacing digits with space
x <- str_replace_all(x, '\\b\\w{1,2}\\b',' ') # replacing one and two letter words with space
x <- str_replace_all(x, '[:punct:]', ' ') # removing punctuation
x <- str_replace_all(x, '\\s+', ' ') # replacing whitespace with space
}
tok_fun <- function(x){tokenize_words(x, stopwords = stopwords())} # removing stopwords
#
it_train <- itoken(amazon_sentences$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = amazon_sentences$id,
progressbar = TRUE)
vocab <- create_vocabulary(it_train)
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 5,
doc_proportion_max = 0.5,
doc_proportion_min = 0.001)
vectorizer <- vocab_vectorizer(pruned_vocab)
t1 <- Sys.time()
# loading data
amazon_sentences <- fread("sentiment labelled sentences/amazon_cells_labelled.txt",
header = FALSE, sep = "\t")
setwd("~/UvA/Subjects/Machine Learning for Econometrics/Lab sessions/Week5")
# loading data
amazon_sentences <- fread("sentiment labelled sentences/amazon_cells_labelled.txt",
header = FALSE, sep = "\t")
colnames(amazon_sentences) <- c("text", "sentiment")
amazon_sentences$id <- rownames(amazon_sentences)
# dtm
prep_fun <- function(x) {
x <- tolower(x)
x <- str_replace_all(x, '[:digit:]', ' ') # replacing digits with space
x <- str_replace_all(x, '\\b\\w{1,2}\\b',' ') # replacing one and two letter words with space
x <- str_replace_all(x, '[:punct:]', ' ') # removing punctuation
x <- str_replace_all(x, '\\s+', ' ') # replacing whitespace with space
}
tok_fun <- function(x){tokenize_words(x, stopwords = stopwords())} # removing stopwords
#
it_train <- itoken(amazon_sentences$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = amazon_sentences$id,
progressbar = TRUE)
vocab <- create_vocabulary(it_train)
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 5,
doc_proportion_max = 0.5,
doc_proportion_min = 0.001)
vectorizer <- vocab_vectorizer(pruned_vocab)
t1 <- Sys.time()
dtm_train <- create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
tfidf <- TfIdf$new()
dtm_tfidf <- fit_transform(dtm_train, tfidf)
# training/testing data
dtm_tfidf <- as.matrix(dtm_tfidf)
data <- as.data.frame(cbind(as.factor(amazon_sentences$sentiment), dtm_tfidf))
names(data)[1] <- "sentiment"
rnd <- runif(1000)
as.factor(amazon_sentences$sentiment)
data$sentiment <- as.factor(data$sentiment)
d_train <- data[rnd < 0.8, ]
d_test <- data[rnd >= 0.8, ]
# SVM
model <- ksvm(sentiment ~ ., data = d_train, kernel = "vanilladot", kpar = "automatic",
C = 10, cross = 10)
# Q1
model@nSV
# predict
pred <- predict(model, d_test[, - 1])
# accuracy
accu <- sum(pred == d_test$sentiment) / dim(d_test)[1] * 100
print(paste("The accuracy is ", accu, "%"))
# costs
mis_cost <- 10 ^ seq(- 2, 2, by = 0.1)
# costs
mis_cost <- 10 ^ seq(- 2, 2, by = 0.1)
pred_accu <- function(miscost){
mod <- ksvm(sentiment ~ ., data = d_train,
kernel = "vanilladot", C = miscost, cross = 10)
return(sum(pred == d_test$sentiment) / dim(d_test)[1] * 100)
}
VDP <- sapply(mis_cost,pred_accu)
VDP
# costs
mis_cost <- 10 ^ seq(- 10, 2, by = 0.5)
pred_accu <- function(miscost){
mod <- ksvm(sentiment ~ ., data = d_train,
kernel = "vanilladot", C = miscost, cross = 10)
return(sum(pred == d_test$sentiment) / dim(d_test)[1] * 100)
}
VDP <- sapply(mis_cost,pred_accu)
VDP
pruned_vocab <- prune_vocabulary(vocab,
term_count_min = 3,
doc_proportion_max = 0.5,
doc_proportion_min = 0.001)
vectorizer <- vocab_vectorizer(pruned_vocab)
t1 <- Sys.time()
dtm_train <- create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
tfidf <- TfIdf$new()
dtm_tfidf <- fit_transform(dtm_train, tfidf)
# training/testing data
dtm_tfidf <- as.matrix(dtm_tfidf)
data <- as.data.frame(cbind(as.factor(amazon_sentences$sentiment), dtm_tfidf))
names(data)[1] <- "sentiment"
rnd <- runif(1000)
data$sentiment <- as.factor(data$sentiment)
d_train <- data[rnd < 0.8, ]
d_test <- data[rnd >= 0.8, ]
# SVM
model <- ksvm(sentiment ~ ., data = d_train, kernel = "vanilladot", kpar = "automatic",
C = 10, cross = 10)
# Q1
model@nSV
# predict
pred <- predict(model, d_test[, - 1])
# accuracy
accu <- sum(pred == d_test$sentiment) / dim(d_test)[1] * 100
print(paste("The accuracy is ", accu, "%"))
# costs
mis_cost <- 10 ^ seq(- 10, 2, by = 0.5)
pred_accu <- function(miscost){
mod <- ksvm(sentiment ~ ., data = d_train,
kernel = "vanilladot", C = miscost, cross = 10)
return(sum(pred == d_test$sentiment) / dim(d_test)[1] * 100)
}
VDP <- sapply(mis_cost, pred_accu)
VDP
10 ^ seq(- 20, 2, by = 0.5)
# costs
mis_cost <- 10 ^ seq(- 2, 2, by = 0.05)
pred_accu <- function(miscost){
mod <- ksvm(sentiment ~ ., data = d_train,
kernel = "vanilladot", C = miscost, cross = 10)
return(sum(pred == d_test$sentiment) / dim(d_test)[1] * 100)
}
VDP <- sapply(mis_cost, pred_accu)
# costs
mis_cost <- 10 ^ seq(- 2, 2, by = 0.1)
pred_accu <- function(miscost){
mod <- ksvm(sentiment ~ ., data = d_train,
kernel = "vanilladot", C = miscost, cross = 10)
pred1 <- predict(mod, d_test[, - 1])
return(sum(pred1 == d_test$sentiment) / dim(d_test)[1] * 100)
}
VDP <- sapply(mis_cost, pred_accu)
VDP
library(kernlab)
# Load other libraries
library(data.table)
library(ggplot2)
library(text2vec)
library(stringr)
library(tokenizers)
library(mlr)
set.seed(150)
# tuning by MLR
classif.train <- makeClassifTask(data = d_train, target = "sentiment")
classif.test <- makeClassifTask(data = d_test, target = "sentiment")
ps <- makeParamSet(
makeNumericParam("C", lower = -10, upper = 6, trafo = function(x) 2^x),
makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot")),
makeNumericParam("sigma", lower = -10, upper = 6, trafo = function(x) 2^x,
requires = quote(kernel == "rbfdot")),
makeIntegerParam("degree", lower = 2L, upper = 5L,
requires = quote(kernel == "polydot"))
)
ctrl <- makeTuneControlGrid()
rdesc <- makeResampleDesc("Holdout")
t2 <- Sys.time()
res <- tuneParams("classif.ksvm", task = classif.train, resampling = rdesc,
par.set = ps, control = ctrl, measures = list(acc))
warnings()
rdesc <- makeResampleDesc("CV", iters = 10L)
t2 <- Sys.time()
res <- tuneParams("classif.ksvm", task = classif.train, resampling = rdesc,
par.set = ps, control = ctrl, measures = list(acc))
warnings()
rdesc <- cv10
res$x
train(makeLearner("classif.ksvm"), classif.train)
mod <- train(makeLearner("classif.ksvm"), classif.train)
mod
mod$dump
mod$learner.model
p <- predict(mod, task = classif.test)
p
sum(p$data$truth == p$data$response)
