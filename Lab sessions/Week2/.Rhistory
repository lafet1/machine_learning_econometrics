job_adverts <- VCorpus(DirSource("vladi"))
library(tm)
getSources()
job_adverts <- VCorpus(DirSource("vladi"))
job_adverts[[1]]$content
job_adverts <- tm_map(job_adverts, content_transformer(tolower))
job_adverts <- tm_map(job_adverts, stripWhitespace)
?tm_map
job_adverts[[1]]$content
dtm <- DocumentTermMatrix(job_adverts,
control = list(
removePunctuation=TRUE,
removeNumbers=TRUE
)
)
?DocumentTermMatrix
dim(dtm)
summary(col_sums(dtm))
library(slam)
summary(col_sums(dtm))
sort(col_sums(dtm), decreasing=TRUE)[1:10]
sort(col_sums(dtm), decreasing=TRUE)[1:15]
findFreqTerms(dtm, lowfreq=100)
findFreqTerms(dtm, lowfreq = 150)
findFreqTerms(dtm, lowfreq = 50)
findAssocs(dtm, terms = "verbal", corlimit = 0.3)
findAssocs(dtm, terms = "verbal", corlimit = 0.5)
findAssocs(dtm, terms = "verbal", corlimit = 0.3)
findAssocs(dtm, terms = "work", corlimit = 0.3)
Terms(dtm)
!(Terms(dtm) %in% findFreqTerms(dtm, lowfreq = 100))
findFreqTerms(dtm, lowfreq = 100)
install.packages("tex2vec")
dtm_new <- dtm[, !(Terms(dtm) %in% findFreqTerms(dtm, lowfreq = 100))] # content-specific "stopwords"
dtm_new <- dtm_new[row_sums(dtm_new) > 0,]
findFreqTerms(dtm_new, lowfreq = 60)
dtm_new$v
?DocumentTermMatrix
dtm_new$dimnames
dtm_new$i
findAssocs(dtm, terms = "the", corlimit = 0.3)
findAssocs(dtm, terms = "arabs", corlimit = 0.3)
row_sums(dtm_new)[dtm_new$i]
head(row_sums(dtm_new)[dtm_new$i])
head(nDocs(dtm_new) / col_sums(dtm_new > 0))
?nDocs
head(col_sums(dtm_new > 0))
?tapply
head(row_sums(dtm_new)[dtm_new$i])
head(dtm_new$v / row_sums(dtm_new)[dtm_new$i])
head(dtm_new$j)
head(tapply(dtm_new$v / row_sums(dtm_new)[dtm_new$i], dtm_new$j, mean))
term_tfidf <- tapply(dtm_new$v / row_sums(dtm_new)[dtm_new$i], dtm_new$j, mean) *
# v is how many times a value occured, we divide how many times the word overall occured
log2(nDocs(dtm_new) / col_sums(dtm_new > 0))
6 * 0.006535948
length(tapply(dtm_new$v / row_sums(dtm_new)[dtm_new$i], dtm_new$j, mean))
length(dtm_new$v / row_sums(dtm_new)[dtm_new$i])
15608/4044
# nDocs gives us the number of documents where the term appears and col_sums gives us the times the term appears
summary(term_tfidf)
dtm_new <- dtm_new[, term_tfidf >= 0.042]
dtm_new <- dtm_new[row_sums(dtm_new) > 0,]
summary(col_sums(dtm_new))
dim(dtm_new)
# some exploring
dim(dtm)
sort(col_sums(dtm), decreasing = TRUE)[1:15] # pick how many most frequent
sort(col_sums(dtm_new), decreasing = TRUE)[1:15] # pick how many most frequent
findFreqTerms(dtm_new, lowfreq = 50) # different frequencies
findAssocs(dtm, terms = "store", corlimit = 0.3) # different words and different correlation limit
# visualization
library(wordcloud)
install.packages("wordlcoud")
install.packages("wordcloud")
# visualization
library(wordcloud)
freq <- data.frame(freqterms = sort(colSums(as.matrix(dtm_new)), decreasing=TRUE))
wordcloud(rownames(freq), freq[, 1], max.words = 50, colors = brewer.pal(3, "Dark2"))
wordcloud(rownames(freq), freq[, 1], max.words = 50, colors = brewer.pal(3, "Dark2"))
findFreqTerms(dtm_new, lowfreq = 50) # different frequencies
sort(col_sums(dtm_new), decreasing = TRUE)[1:15] # pick how many most frequent
