dim(b_ML)
mean(b_ML[, 1])
mean(b_ML[, 2])
print(mean(b_ML[, 1]), mean(b_ML[, 2]), mean(b_NLS[, 1]). mean(b_NLS[, 2]))
print(mean(b_ML[, 1]), mean(b_ML[, 2]), mean(b_NLS[, 1]), mean(b_NLS[, 2]))
mean(b_ML[, 1]
mean(b_ML[, 1])
mean(b_ML[, 2])
mean(b_NLS[, 1])
mean(b_NLS[, 2]
)
fitNLS
# c
fitNLS <- nls(y ~ - exp(X[, 1] * theta1 + X[, 2] * theta2), start = list(theta1 = b[1], theta2 = b[2]))
fitNLS
# c
fitNLS <- nls(y ~  exp(- (X[, 1] * theta1 + X[, 2] * theta2)), start = list(theta1 = b[1], theta2 = b[2]))
fitNLS
print(mean(b_ML[, 1]), mean(b_ML[, 2]), mean(b_NLS[, 1]), mean(b_NLS[, 2]))
mean(b_ML[, 1])
mean(b_ML[, 2])
mean(b_NLS[, 1])
mean(b_NLS[, 2]))
mean(b_NLS[, 2])
beta <- 1
rho <- 0.5
r <- 3
pi_1 <- 1
pi <- c(pi_1, rep(0, r - 1))
pi
gamma_1 <- 0 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
?norm
?rnorm
cbind(rep(rnorm(N, 0, 1), r))
head(cbind(rep(rnorm(N, 0, 1), r)))
head(rep(rnorm(N, 0, 1), r))
beta <- 1
rho <- 0.5
r <- 3
pi_1 <- 1
pi <- c(pi_1, rep(0, r - 1))
gamma_1 <- 0 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
rep(rnorm(N, 0, 1), r)
N <- 100
rep(rnorm(N, 0, 1), r)
head(rep(rnorm(N, 0, 1), r))
dim(rep(rnorm(N, 0, 1), r))
length(rep(rnorm(N, 0, 1), r))
Z <- cbind(rnorm(N, 0, 1), r), rnorm(N, 0, 1), r), rnorm(N, 0, 1), r))
Z <- matrix(rnorm(N, 0, 1), N, r)
sigma <- exp(Z %*% gamma)
R <- 200
b_2SLS <- matrix(sample(0, R, replace = T), R, 1)
b_GMM <- matrix(sample(0, R, replace = T), R, 1)
b_EGMM <- matrix(sample(0, R, replace = T), R, 1)
b_OLS <- matrix(sample(0, R, replace = T), R, 1)
beta <- 1
rho <- 0.5
r <- 3
pi_1 <- 1
pi <- c(pi_1, rep(0, r - 1))
gamma_1 <- 0 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
N <- 100
Z <- matrix(rnorm(N, 0, 1), N, r)
sigma <- exp(Z %*% gamma)
# matrices for results
R <- 200
b_2SLS <- matrix(sample(0, R, replace = T), R, 1)
b_GMM <- matrix(sample(0, R, replace = T), R, 1)
b_EGMM <- matrix(sample(0, R, replace = T), R, 1)
b_OLS <- matrix(sample(0, R, replace = T), R, 1)
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- lm(y ~ x)
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% Z %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- coef(lm(y ~ x))
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% Z %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
coef(lm(y ~ x))
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- coef(lm(y ~ x))[2]
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% Z %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
t(Z) %*% Z
Z <- cbin(rnorm(N, 0, 1), rnorm(N, 0, 1), rnorm(N, 0, 1))
Z <- cbind(rnorm(N, 0, 1), rnorm(N, 0, 1), rnorm(N, 0, 1))
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- coef(lm(y ~ x))[2]
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% Z %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
t(Z) %*% Z
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- coef(lm(y ~ x))[2]
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(b_OLS)
coef(lm(y ~ x))[2]
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
as.numeric(coef(lm(y ~ x))[2])
b_2SLS <- matrix(sample(0, R, replace = T), R, 1)
b_GMM <- matrix(sample(0, R, replace = T), R, 1)
b_EGMM <- matrix(sample(0, R, replace = T), R, 1)
b_OLS <- matrix(sample(0, R, replace = T), R, 1)
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(b_OLS)
mean(b_2SLS)
gamma_1 <- 0.5 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
N <- 100
Z <- cbind(rnorm(N, 0, 1), rnorm(N, 0, 1), rnorm(N, 0, 1))
sigma <- exp(Z %*% gamma)
# matrices for results
R <- 200
b_2SLS <- matrix(sample(0, R, replace = T), R, 1)
b_GMM <- matrix(sample(0, R, replace = T), R, 1)
b_OLS <- matrix(sample(0, R, replace = T), R, 1)
b_EGMM <- matrix(sample(0, R, replace = T), R, 1)
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(b_2SLS)
mean(e)
gamma_1 <- 0 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(e)
gamma_1 <- 1 # heteroskedasticity parameter
gamma <- c(gamma_1, rep(0, r -1))
for (i in 1:R){
u <- rnorm(N, 0, sigma)
v <- (u / sigma) * rho + rnorm(N, 0, 1) * sqrt(1 - rho^2)
x <- Z %*% pi + v
y <- x %*% beta + u
b_OLS[i] <- as.numeric(coef(lm(y ~ x))[2])
b_2SLS[i] <- solve(t(x) %*% Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% x) %*% t(x) %*% Z %*% solve(t(Z) %*% Z) %*%
t(Z) %*% y
e <- y - x %*% b_2SLS[i]
}
mean(e)
library(tidyverse)
library(installr)
install.packages("installr")
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("dplyr")
library(tidyverse)
install.packages("tidyverse")
install.packages("tm")
install.packages("mgcv")
install.packages("readr")
install.packages("RCurl")
knitr::opts_chunk$set(echo = F,	message = FALSE,warning = FALSE)
library(tidyverse)
ggplot(data = iris, aes(x = Sepal.Width, fill = Species)) +
geom_histogram(binwidth = 0.2, col = "black", size = 1) +
labs(x = "Sepal width", y = "Frequency", title = "Histogram of Sepal width")
knitr::opts_chunk$set(echo = F,	message = FALSE,warning = FALSE)
ggplot(data = iris, aes(x = Sepal.Length, fill = Species, col = Species)) +
geom_density(alpha = 0.5, size = 0.5) +
labs(x = "Sepal length", y = "Density", title = "Iris dataset:
Smoothed density of Sepal Length per Species")
knitr::opts_chunk$set(echo = F,	message = FALSE,warning = FALSE)
ggplot(data = iris, aes(x = Sepal.Length, y = iris$Sepal.Width)) +
geom_point(aes(x = iris$Sepal.Length, y = iris$Sepal.Width, fill = iris$Species,
col = iris$Species, shape = iris$Species),
size = 2) +
geom_smooth(method = lm, aes(x = iris$Sepal.Length,
y = iris$Sepal.Width, col = iris$Species)) +
labs(x = "Sepal length", y = "Sepal Width", title = "Scatterplot with smoothers")
library(tm)
getSources()
setwd()
library(keras)
library(glmnet)
install.packages("dbscan")
install.packages("e1071")
library(dbscan)
library(e1071)
?dbscan
?svm
install.packages("LDAvis")
library(data.table) # load and transform data
library(tidyverse) # pipes, plots and other nice stuff
library(tm) # stop words
library(text2vec) # heavy lifting of text analysis
library(LDAvis) # visualize topic models
# load data from past weeks:
amazon_sentences <- fread("sentiment labelled sentences/amazon_cells_labelled.txt", header=FALSE, sep="\t")
amazon_sentences$Source <- "Amazon"
yelp_sentences <- fread("yelp_labelled.txt", header=FALSE, sep="\t")
yelp_sentences$Source <- "Yelp"
imdb_sentences <- fread("imdb_labelled.txt", header=FALSE, sep="\t")
imdb_sentences$Source <- "Imdb"
setwd("~/UvA/Subjects/Machine Learning for Econometrics/Lab sessions/Week6")
# load data from past weeks:
amazon_sentences <- fread("sentiment labelled sentences/amazon_cells_labelled.txt", header=FALSE, sep="\t")
amazon_sentences$Source <- "Amazon"
amazon_sentences$Source <- "Amazon"
yelp_sentences <- fread("sentiment labelled sentences/yelp_labelled.txt", header=FALSE, sep="\t")
yelp_sentences$Source <- "Yelp"
imdb_sentences <- fread("sentiment labelled sentences/imdb_labelled.txt", header=FALSE, sep="\t")
imdb_sentences$Source <- "Imdb"
# select which documents to include in our analysis try out different (combinations of sets and see how results vary)
alltext <- rbind(yelp_sentences)
# convert to data.table
alltext <- alltext %>%
mutate(id = rownames(alltext)) %>%
rename(review = V1, sentiment = V2)
# rename cols
setDT(alltext)
setkey(alltext, id)
# data split
set.seed(5) #to reproduce results
# train test split
# Use alltext[J(train_ids)] / alltext[J(test_ids)] for each set
all_ids <- alltext$id
train_ids <- sample(all_ids, round(nrow(alltext)*0.8))
test_ids <- setdiff(all_ids, train_ids)
# DTM
# this week we show you the text2vec approach which has a nice implementation of LDA
tokens = alltext[J(train_ids)]$review %>%
tolower %>%
word_tokenizer
it = itoken(tokens, ids = alltext[J(train_ids)]$id, progressbar = FALSE)
# we still use tm package's stopwords though
stop_words <- stopwords('en')
# account for typo's by also adding those words without ' as stopwords and where they are separated by a space
new_stop_words <- stop_words
for(word in stop_words){
split <- strsplit(word,"'")[[1]]
if(length(split) > 1){
for(w in split){
new_stop_words <- c(new_stop_words,w)
}
new_stop_words <- c(new_stop_words,paste(split,collapse=''))
}
}
new_stop_words <- unique(new_stop_words)
# create vocabulary
v = create_vocabulary(it,stopwords = new_stop_words) %>%
prune_vocabulary(term_count_min = 5, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
# create the dtm
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
# Create your first LDA Model
n_topics = 20
lda_model = LDA$new(n_topics = n_topics, doc_topic_prior = 1/n_topics,
topic_word_prior = 1/length(v[[1]]))
# calc doc topic matrix
doc_topic_distr =
lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
doc_id <- 39
title <- print(alltext[train_ids[doc_id]]$review)
barplot(doc_topic_distr[doc_id, ], xlab = "topic",
ylab = "proportion", ylim = c(0, 1),
names.arg = 1:ncol(doc_topic_distr))
# to inspect the full topic word matrix use : lda_model$topic_word_distribution
lda_model$get_top_words(n = 10, topic_number = c(9,20), lambda = 1)
lda_model$plot()
##### Part 2 #####
doc_id <- 10
title <- print(alltext[train_ids[doc_id]]$review)
##### Part 2 #####
doc_id <- 1000
title <- print(alltext[train_ids[doc_id]]$review)
##### Part 2 #####
doc_id <- 100
title <- print(alltext[train_ids[doc_id]]$review)
barplot(doc_topic_distr[doc_id, ], xlab = "topic",
ylab = "proportion", ylim = c(0, 1),
names.arg = 1:ncol(doc_topic_distr))
##### Part 2 #####
doc_id <- 15
title <- print(alltext[train_ids[doc_id]]$review)
barplot(doc_topic_distr[doc_id, ], xlab = "topic",
ylab = "proportion", ylim = c(0, 1),
names.arg = 1:ncol(doc_topic_distr))
##### Part 3 #####
# create the dtm of the test set
new_dtm = itoken(alltext[J(test_ids)]$review, tolower, word_tokenizer, ids = alltext[J(test_ids)]$id) %>%
create_dtm(vectorizer, type = "dgTMatrix")
# estimate the document topic distribution of this new text based on the old model
new_doc_topic_distr = lda_model$transform(new_dtm)
# calculate perplexity
perp <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)
print(perp)
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt = i
lda = LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
perp_alt[i] <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution,
doc_topic_distribution = new_doc_topic_distr)
}
perp_alt
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt = i
lda = LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
perp_alt[i] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = new_doc_topic_distr)
}
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt = i
lda = LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt =
lda$fit_transform(x = new_dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
perp_alt[i] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = doc_topic_distr_alt)
}
perp_alt
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt = i
lda = LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt =
lda$fit_transform(x = new_dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
perp_alt[i] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = new_doc_topic_distr)
}
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt = i
lda = LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt =
lda$fit_transform(x = new_dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
perp_alt[i] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = doc_topic_distr_alt)
}
perp_alt
n_topics_alt <- 10
lda <- LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt <-
lda$fit_transform(x = new_dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = doc_topic_distr_alt)
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt <- 10
lda <- LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt <-
lda$fit_transform(x = new_dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
perp_alt[i/5 - 1] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = doc_topic_distr_alt)
}
perp_alt
length(v[[1]])
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt <- 10
lda <- LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt <-
lda$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
perp_alt[i/5 - 1] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = doc_topic_distr_alt)
}
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt <- 10
lda <- LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt <-
lda$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
new_doc_topic_distr_alt <- lda_model$transform(new_dtm)
perp_alt[i/5 - 1] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = new_doc_topic_distr_alt)
}
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt <- 10
lda <- LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt <-
lda$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
new_doc_topic_distr_alt <- lda$transform(new_dtm)
perp_alt[i/5 - 1] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = new_doc_topic_distr_alt)
}
perp_alt
perp_alt <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt <- i
lda <- LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr_alt <-
lda$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
new_doc_topic_distr_alt <- lda$transform(new_dtm)
perp_alt[i/5 - 1] <- perplexity(new_dtm, topic_word_distribution = lda$topic_word_distribution,
doc_topic_distribution = new_doc_topic_distr_alt)
}
perp_alt
plot(perp_alt, type = "l")
new_doc_topic_distr_alt
dim(new_doc_topic_distr_alt)
dim(lda$topic_word_distribution)
# Create your first LDA Model
n_topics = 20
sum(doc_topic_distr == 0)
sum(doc_topic_distr == 0) / (nrow(doc_topic_distr) * ncol(doc_topic_distr))
nrow(doc_topic_distr)
ncol(doc_topic_distr)
doc_topic <- vector()
for (i in seq(10, 100, by = 5)){
n_topics_alt <- i
lda1 <- LDA$new(n_topics = n_topics_alt, doc_topic_prior = 1/n_topics_alt,
topic_word_prior = 1/length(v[[1]]))
doc_topic_distr1 <-
lda$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
doc_topic[i/5 - 1] <- sum(doc_topic_distr1 == 0) / (nrow(doc_topic_distr1) * ncol(doc_topic_distr1))
}
doc_topic
sum(doc_topic_distr1 == 0)
(nrow(doc_topic_distr1) *
ncol(doc_topic_distr1))
